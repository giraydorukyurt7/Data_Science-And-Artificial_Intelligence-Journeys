{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54902f7b",
   "metadata": {},
   "source": [
    "##### Markov Model Captures \"Flow,\" Not \"Meaning\"\n",
    "\n",
    "The Markov model calculates the probability of which word is likely to follow another  \n",
    "($P(w_t \\mid w_{t-1})$). An author's signature is not only the words they use, but how they  \n",
    "structure them.\n",
    "\n",
    "- **Poe**: \"The bird hath flown\" (archaic English usage)  \n",
    "- **Frost**: \"The bird has flown\" (modern usage)\n",
    "\n",
    "If you apply lemmatization:  \n",
    "- *hath* → *have*  \n",
    "- *has* → *have*  \n",
    "- *flown* → *fly*  \n",
    "\n",
    "Both sentences collapse into: **\"the bird have fly\"**.  \n",
    "This destroys the author's unique grammatical style (their *voice*), making it harder for the model  \n",
    "to distinguish between Poe and Frost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a7417c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de0e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(TXT_DIR):\n",
    "    line_list = []\n",
    "    TXT_DIR = \"../\"+TXT_DIR\n",
    "    with open(TXT_DIR, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            #normalization\n",
    "            line = line.strip()\n",
    "            line = line.lower()\n",
    "            line = re.sub(r'[^\\w\\s]', '', line) #substitute (replace)\n",
    "            line = re.sub(r'\\d', '', line)\n",
    "            if line != \"\":\n",
    "                tokens = line.split()\n",
    "                line_list.append(tokens) #Markov models want this format. \n",
    "    return line_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6745da4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "poe_list = get_data('data/edgar_allan_poe.txt')\n",
    "frost_list = get_data('data/robert_frost.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "de5f14fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "718\n",
      "1436\n"
     ]
    }
   ],
   "source": [
    "print(len(poe_list))\n",
    "print(len(frost_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2affda7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2154\n",
      "2154\n"
     ]
    }
   ],
   "source": [
    "# for poe = 0 and frost = 1 mapping\n",
    "inputs = poe_list + frost_list\n",
    "labels = [0]*len(poe_list) + [1]*len(frost_list)\n",
    "\n",
    "print(len(inputs))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6839df41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1723 - 1723\n",
      "431 - 431\n",
      "['not', 'lupine', 'living', 'on', 'sand', 'and', 'drouth'] - 1\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(inputs, labels, shuffle=True, train_size=0.80, test_size=0.20, random_state=42)\n",
    "\n",
    "print(len(X_train), \"-\", len(y_train))\n",
    "print(len(X_test), \"-\", len(y_test))\n",
    "print(X_train[100], \"-\", y_train[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366c7035",
   "metadata": {},
   "source": [
    "* in markov models we need to map unique words into integers, so it can procude it.\n",
    "* since in real world we can have unknown words, we should only use X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad25164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2604\n"
     ]
    }
   ],
   "source": [
    "#Old code:\n",
    "\n",
    "#all_x_train = \"\"\n",
    "#for sentence in X_train:\n",
    "#    all_x_train += ' '.join(sentence) + ' '\n",
    "#unique_words = list(set(all_x_train.split()))\n",
    "#unique_words.insert(0, '<UNKNOWN>')\n",
    "##create a unique word dict\n",
    "#word2idx = {word: i for i, word in enumerate(unique_words)}\n",
    "#\n",
    "#print(len(word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "96bbf24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2604\n",
      "[('<UNKNOWN>', 0), ('grace', 1), ('mosses', 2), ('ghastly', 3), ('point', 4)]\n"
     ]
    }
   ],
   "source": [
    "#Since strings in python are immutable, editing is slow\n",
    "#Faster way:\n",
    "\n",
    "#create empty set\n",
    "word_set = set()\n",
    "#directly add words into set\n",
    "for sentence in X_train:\n",
    "    word_set.update(sentence)\n",
    "idx2word = list(word_set)\n",
    "idx2word.insert(0, '<UNKNOWN>')\n",
    "\n",
    "word2idx = {word:i for i, word in enumerate(idx2word)}\n",
    "print(len(word2idx))\n",
    "print(list(word2idx.items())[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7f04ee19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'many', 'must', 'have', 'seen', 'him', 'make']\n",
      "[2086, 445, 94, 2076, 641, 117, 1232]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "print([word2idx[word] for word in X_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9276cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2086, 445, 94, 2076, 641, 117, 1232, 0, 445],\n",
       " [2457, 0, 838, 1540, 2257, 1052, 155, 2508, 403]]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##turn inputs into numbers\n",
    "#\n",
    "#test_idx = []\n",
    "#for sentence in test_list:\n",
    "#    sample = []\n",
    "#    for word in sentence:\n",
    "#        try:            \n",
    "#            sample.append(word2idx[word])\n",
    "#        except:\n",
    "#            sample.append(0)\n",
    "#    test_idx.append(sample)\n",
    "#test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "870319b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_numerizer(arr):\n",
    "    int_list = []\n",
    "    for sentence in arr:\n",
    "        sample = [word2idx.get(word, 0) for word in sentence]\n",
    "        int_list.append(sample)\n",
    "    return int_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4398074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_int = word_numerizer(X_train)\n",
    "X_test_int  = word_numerizer(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "753d4f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'joy', 'and', 'wo', 'in', 'good', 'and', 'ill']\n",
      "[2508, 2207, 2086, 0, 2508, 434, 2086, 929]\n"
     ]
    }
   ],
   "source": [
    "print(X_test[10])\n",
    "print(X_test_int[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfb6040",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
