{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8497763d",
   "metadata": {},
   "source": [
    "# Generative Models by using Markov Models\n",
    "\n",
    "**Grandfather of Chatgpt**\n",
    "\n",
    "### Differences between classification and generative models:\n",
    "* No Matrices, Only a Dictionary: We will be working with sparse data. Using matrices will bloat the memory because most words don't follow each other. We will only keep track of \"existing\" transitions. \n",
    "* No Smoothing: This is very important. When creating classifiers, we said \"let's give a chance to the unseen.\" But when writing poetry, we don't want made-up words. We want the model to only make the actual transitions it sees. \n",
    "* No Logarithms: To be able to sample, we need real probabilities between $0$ and $1$. (e.g., 20% probability, 50% probability). Logarithms disrupt this ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "750371f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "3c3fa729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(TXT_DIR):\n",
    "    line_list = []\n",
    "    TXT_DIR = \"../\"+TXT_DIR\n",
    "    with open(TXT_DIR, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            #normalization\n",
    "            line = line.strip()\n",
    "            line = line.lower()\n",
    "            line = re.sub(r'[^\\w\\s]', '', line) #substitute (replace)\n",
    "            line = re.sub(r'\\d', '', line)\n",
    "            if line != \"\":\n",
    "                tokens = line.split()\n",
    "                line_list.append(tokens) #Markov models want this format. \n",
    "    return line_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "bc0d0016",
   "metadata": {},
   "outputs": [],
   "source": [
    "frost_list = get_data('data/robert_frost.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "f8ce2511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2192\n",
      "[('<UNKNOWN>', 0), ('chanced', 1), ('straight', 2), ('maybe', 3), ('half', 4)]\n"
     ]
    }
   ],
   "source": [
    "#Since strings in python are immutable, editing is slow\n",
    "#Faster way:\n",
    "\n",
    "#create empty set\n",
    "word_set = set()\n",
    "#directly add words into set\n",
    "for sentence in frost_list:\n",
    "    word_set.update(sentence)\n",
    "idx2word = list(word_set)\n",
    "idx2word.insert(0, '<UNKNOWN>')\n",
    "\n",
    "word2idx = {word:i for i, word in enumerate(idx2word)}\n",
    "print(len(word2idx))\n",
    "print(list(word2idx.items())[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "3bcc26ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_numerizer(arr):\n",
    "    int_list = []\n",
    "    for sentence in arr:\n",
    "        sample = [word2idx.get(word, 0) for word in sentence]\n",
    "        int_list.append(sample)\n",
    "    return int_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "a1c57aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "frost_list_int = word_numerizer(frost_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "8bbe60b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'both', 'that', 'morning', 'equally', 'lay']\n",
      "[895, 325, 611, 1713, 1685, 1625]\n"
     ]
    }
   ],
   "source": [
    "print(frost_list[10])\n",
    "print(frost_list_int[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "4f803c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare A1, A2 and pi\n",
    "\n",
    "pi = {}\n",
    "A1 = {}\n",
    "A2 = {}\n",
    "\n",
    "for sentence in frost_list_int: \n",
    "    #pi\n",
    "    if sentence[0] not in pi:\n",
    "        pi[sentence[0]] = 1\n",
    "    else:\n",
    "        pi[sentence[0]] +=1\n",
    "    #A1\n",
    "    if len(sentence) >=2:\n",
    "        first_word = sentence[0]\n",
    "        second_word = sentence[1]\n",
    "        if first_word not in A1:\n",
    "            A1[first_word] = {}\n",
    "            A1[first_word][second_word] = 1\n",
    "        else:\n",
    "            if second_word not in A1[first_word]:\n",
    "                A1[first_word][second_word] = 1\n",
    "            else:\n",
    "                A1[first_word][second_word] +=1 \n",
    "    #A2\n",
    "    for word_index in range(len(sentence)-2):\n",
    "        #key\n",
    "        current_word = sentence[word_index]\n",
    "        second_word  = sentence[word_index+1]\n",
    "        third_word   = sentence[word_index+2]\n",
    "\n",
    "        keytuple = (current_word, second_word)\n",
    "\n",
    "        if keytuple not in A2:\n",
    "            A2[keytuple] = {}\n",
    "            A2[keytuple][third_word] = 1                    \n",
    "        else:\n",
    "            if third_word not in A2[keytuple]:\n",
    "                A2[keytuple][third_word]= 1\n",
    "            else:\n",
    "                A2[keytuple][third_word]+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "762c33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrices as probabilities\n",
    "\n",
    "# FOR pi\n",
    "def calc_pi_probs(dict_):\n",
    "    #1- Not using log probabilities. We need real probabilities\n",
    "    #2- No smoothing (+1). Model will not use word combinations \n",
    "    # that never appeared in training test. \n",
    "    new_pi = {}\n",
    "    total_count = 0\n",
    "    for val in dict_.values():\n",
    "        total_count += val\n",
    "\n",
    "    for key, value in dict_.items():\n",
    "        new_pi[key] = (value)/(total_count)\n",
    "    return new_pi\n",
    "\n",
    "# FOR A\n",
    "def calc_A_probsprobabilities(dict_):\n",
    "    new_A = {}\n",
    "    \n",
    "    for key in dict_.keys():\n",
    "        new_pi = calc_pi_probs(dict_[key])\n",
    "        new_A[key] = new_pi\n",
    "    \n",
    "    return new_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "66f388d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_probs = calc_pi_probs(pi)\n",
    "A1_probs = calc_A_probsprobabilities(A1)\n",
    "A2_probs = calc_A_probsprobabilities(A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "5f33474d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([618, 895, 768, 1643, 559, 1237, 581, 1641, 2079, 64, 1992, 704, 1367, 942, 991, 836, 385, 1454, 1476, 1488, 1954, 791, 74, 479, 1036, 882, 1383, 243, 971, 1072, 1492, 147, 4, 2060, 1797, 881, 2038, 1798, 1513, 616, 173, 75, 1313, 2100, 399, 551, 1274, 1431, 1892, 611, 1473, 320, 1259, 1002, 1676, 659, 1210, 1126, 669, 1331, 55, 719, 277, 1737, 760, 1542, 2071, 306, 1382, 1510, 1319, 936, 774, 433, 2004, 174, 2024, 841, 19, 184, 707, 1832, 1158, 2036, 1429, 1439, 68, 126, 1770, 1329, 1043, 2005, 45, 1493, 1495, 1167, 749, 998, 2085, 624, 207, 1778, 970, 537, 291, 48, 2054, 262, 1762, 2094, 228, 11, 1842, 2093, 835, 224, 415, 1704, 1974, 891, 1609, 872, 924, 851, 598, 1764, 1748, 357, 272, 2072, 1800, 1215, 893, 1822, 1272, 52, 1066, 1853, 785, 1662, 1813, 1385, 514, 352, 2162, 1055, 1574, 325, 427, 444, 378, 413, 278, 798, 24, 331, 914, 1717, 1006, 1276, 59, 549, 1784, 222, 1780, 345, 423, 436, 682, 2063, 1695, 1895, 645, 1350, 570, 1556, 990, 1939, 1039, 1355, 2092, 916, 773, 58, 327, 335, 294, 1093, 1025, 1373, 2008, 2151, 1226, 2051, 2059, 670, 246, 1519, 1845, 1357, 1464, 1537, 313, 1585, 256, 500, 754, 358, 1571, 756, 1712, 1772, 1349, 2173, 1074, 1833, 566, 985, 1776, 360, 8, 1112, 2189, 138, 565, 194, 575, 1614, 16, 1725, 223, 1732, 751, 1517, 556, 132, 164, 1456, 738, 1345, 1752, 2057, 1936, 1547, 493, 300, 1795, 163, 699, 1354, 2123, 877, 466, 1119, 2179, 1827, 379, 1131, 612, 1041, 906, 1965, 610, 192, 1619, 764, 2084, 1779, 98, 1498, 1008, 1474, 1230, 1197, 290, 1979, 145, 409, 1709, 1487, 508, 78, 111, 447, 2115, 317, 1986, 416, 1042, 2158, 226, 1860, 868, 807, 592, 1808, 271, 1621, 652, 1525, 589, 737, 1789, 1848, 183])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_probs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "b48a55a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select random first word:\n",
    "\n",
    "first_word = np.random.choice([*pi_probs.keys()], p=[*pi_probs.values()])\n",
    "second_word = np.random.choice([*A1_probs[first_word].keys()], p=[*A1_probs[first_word].values()])\n",
    "search_key = (first_word, second_word)\n",
    "third_word = np.random.choice([*A2_probs[search_key].keys()], p=[*A2_probs[search_key].values()])\n",
    "search_key = (second_word, third_word)\n",
    "fourth_word = np.random.choice([*A2_probs[search_key].keys()], p=[*A2_probs[search_key].values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "7e68e791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1510\n",
      "611\n",
      "1431\n",
      "970\n"
     ]
    }
   ],
   "source": [
    "print(first_word)\n",
    "print(second_word)\n",
    "print(third_word)\n",
    "print(fourth_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "bbd5c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {}\n",
    "for v, i in word2idx.items():\n",
    "    idx2word[i] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "a4b7ee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poem_int(POEM_SIZE = 4):\n",
    "    POEM = []\n",
    "    while True:\n",
    "        poem_tokens = []\n",
    "        while True:\n",
    "            first_word = np.random.choice([*pi_probs.keys()], p=[*pi_probs.values()])\n",
    "            if first_word in A1_probs:\n",
    "                second_word = np.random.choice([*A1_probs[first_word].keys()], p=[*A1_probs[first_word].values()])\n",
    "                break\n",
    "        poem_tokens = [first_word, second_word]\n",
    "        while len(poem_tokens)<6:\n",
    "            search_key = (poem_tokens[-2], poem_tokens[-1])\n",
    "            if search_key in A2_probs:\n",
    "                next_word = np.random.choice([*A2_probs[search_key].keys()], p=[*A2_probs[search_key].values()])\n",
    "                poem_tokens.append(next_word)\n",
    "            else:\n",
    "                break\n",
    "        if len(poem_tokens) == 6:\n",
    "            POEM.append(poem_tokens)\n",
    "            if len(POEM) == POEM_SIZE:\n",
    "                break\n",
    "    return POEM\n",
    "\n",
    "def generate_poem(POEM_SIZE = 4):\n",
    "    POEM = generate_poem_int(POEM_SIZE)\n",
    "    POEM_str = \"\"\n",
    "    for line in POEM:\n",
    "        poem_line = []\n",
    "        for token in line:\n",
    "            poem_line.append(idx2word[token])\n",
    "        POEM_str += ' '.join(poem_line) + '\\n'\n",
    "    return POEM_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "1958ed21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you and i know enough of\n",
      "i take it as i am\n",
      "to stand together on the cellar\n",
      "strung on your hair wont hurt\n",
      "\n",
      "i wonder if hes a day\n",
      "i stole the goblet from the\n",
      "one level higher than the run\n",
      "let them will we come to\n",
      "\n",
      "weep for what little things could\n",
      "son you wouldnt want to know\n",
      "yes its important though you think\n",
      "the origin of all her books\n",
      "\n",
      "or i cant give myself to\n",
      "and in a wood and i\n",
      "son but the pipes there and\n",
      "especially in winter when the wind\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_poem())\n",
    "print(generate_poem())\n",
    "print(generate_poem())\n",
    "print(generate_poem())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "a38f0979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a moment he stood beside and\n",
      "a chimney that only would serve\n",
      "old davis owned a solid mica\n",
      "i was young i used to\n",
      "\n",
      "all fresh and sound from the\n",
      "as married thats what i came\n",
      "they bad been brought home from\n",
      "and kick with two legs like\n",
      "\n",
      "if you can be or anyone\n",
      "that everyone for miles could see\n",
      "two roads diverged in a town\n",
      "moving a flock of hens from\n",
      "\n",
      "its as sound as the day\n",
      "then shook his lantern saying iles\n",
      "so now theyve dragged it through\n",
      "up one flight from the sense\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_poem())\n",
    "print(generate_poem())\n",
    "print(generate_poem())\n",
    "print(generate_poem())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
